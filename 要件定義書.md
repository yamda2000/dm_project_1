## 要件定義書（本アプリ実装に基づく）

### 1. プロジェクト概要

#### 1.1 目的
社内のローカルドキュメントおよび指定のWebページを参照し、質問に回答するRAG型のFAQチャットボットを提供する。Streamlitを用いたWeb UI上で、社内文書検索モードと社内問い合わせモードの2形態で回答を行う。

#### 1.2 スコープ
- **Web UI**: StreamlitベースのチャットUI（`st.chat_input`、会話履歴表示、サイドバーのモード切替）
- **データ取り込み**: ローカルフォルダ（`./data`）配下のPDF/DOCX/CSV/TXTの自動読み込みと分割・埋め込み・ベクトル化。併せて指定Webページの読み込み
- **検索/回答**: History-aware Retriever + Retrieval Chain による関連ドキュメント検索とLLM回答生成
- **ログ**: 日次ローテーションのファイルログ出力（`./logs/application.log`）


### 2. 機能要件

#### 2.1 ドキュメント管理
- **対応フォーマット**: PDF（PyMuPDFLoader）、DOCX（Docx2txtLoader）、CSV（CSVLoader, utf-8）、TXT（TextLoader, utf-8）、Webページ（`langchain_community.document_loaders.WebBaseLoader`）
- **ファイル配置**: `./data` 直下を再帰的に走査して読み込み
- **更新**: アプリ起動時に読み込み・チャンク分割・埋め込み・ベクトル化を実施し、ベクトルDBをローカル保存（`./FAISS_db`）

#### 2.2 チャット機能
- **入力**: テキストベースの質問入力（`st.chat_input`）
- **モード**:
  - 社内文書検索: 入力と関連性の高いドキュメントの所在を提示
  - 社内問い合わせ: 文脈に基づく回答テキストと参照元の提示
- **処理**:
  - 会話履歴を考慮した独立質問生成（History-aware Retriever）
  - ベクトル検索（FAISS, k=5）
  - LLMによる回答生成（プロンプトはモード別に最適化）
- **出力**:
  - 回答テキスト
  - 参照元ドキュメントのパス（必要に応じてページ番号）
  - 関連度スコアの表示は未実装
- **会話ログ**: 画面表示用とLLM用の履歴をセッションステートで保持

#### 2.3 基本設定
- **LLMモデル**: `gpt-4o-mini`
- **埋め込みモデル**: `OpenAIEmbeddings`（デフォルト設定）
- **チャンクサイズ/オーバーラップ**: 500 / 50
- **検索件数（k）**: 5


### 3. 技術仕様

#### 3.1 必須コンポーネント（主要ライブラリ）
- UI: `streamlit`
- フレームワーク/チェーン: `langchain`, `langchain-openai`, `langchain-community`
- ベクトルDB: `faiss-cpu`（保存先は`./FAISS_db`）
- ドキュメントローダー: `PyMuPDF`（PDF）, `docx2txt`（DOCX）, `langchain_community.csv_loader.CSVLoader`, `langchain_community.TextLoader`
- その他: `python-dotenv`（環境変数）, `uuid`（セッションID）, `logging`（ローテーションログ）

#### 3.2 基本アーキテクチャ
ユーザー入力
↓
独立質問生成（会話履歴考慮）
↓
ベクトル検索（FAISS, k=5）
↓
関連ドキュメント取得（PDF/DOCX/CSV/TXT/指定Web）
↓
LLMによる回答生成（モード別プロンプト）
↓
回答表示（参照元の所在提示を含む）

#### 3.3 ディレクトリ構造（抜粋）
```
project/
├── main.py                # Streamlitメイン
├── initialize.py          # 初期化（読込・分割・埋込・FAISS保存・retriever作成・ログ設定）
├── components.py          # 画面レンダリングと表示整形
├── utils.py               # LLM/RAGチェーン構築・呼び出し
├── constants.py           # 各種定数・プロンプト・設定
├── data/                  # 参照ドキュメント格納フォルダ
├── FAISS_db/              # FAISSベクトルDB保存先
├── logs/                  # アプリログ（TimedRotating）
├── requirements.txt
└── README.md
```


### 4. 実装要件

#### 4.1 基本実装フロー
1) **初期化処理**（`initialize.initialize()`）
   - `./data` を再帰走査し対応拡張子のファイルを各ローダーで読み込み
   - 指定Webページの読み込み（`constants.WEB_URL_LOAD_TARGETS`）
   - Windows環境向け文字列調整（Unicode正規化・cp932不可文字除去）
   - `CharacterTextSplitter` でチャンク分割（size=500, overlap=50）
   - `OpenAIEmbeddings` でベクトル化
   - `FAISS.from_documents()` でベクトルストア作成・`./FAISS_db` に保存
   - `db.as_retriever(search_kwargs={"k": 5})` をセッションに格納
   - ログ設定（`./logs/application.log` に日次ローテーション）

2) **質問応答処理**（`utils.get_llm_response()`）
   - モード別システムプロンプト（文書検索/問い合わせ）
   - History-aware Retriever で独立質問生成
   - Retrieval Chain で文脈を詰めてLLMへ入力
   - LLM応答取得後、会話履歴へ保存（チャット履歴＋回答）
   - モード別の表示整形（`components.display_*`）で回答と参照元を提示

#### 4.2 エラーハンドリング
- **APIキー未設定**: `.env` が無い場合は `st.secrets["OPENAI_API_KEY"]` を参照
- **初期化失敗/回答生成失敗/表示失敗/履歴表示失敗**: ログ出力し、画面にユーザー向けエラー文（共通問い合わせテンプレート連結）を表示して処理中断
- **関連ドキュメントなし**: 文書検索モードでは「該当資料なし」、問い合わせモードでは「回答に必要な情報が見つかりませんでした。」の文面で通知


### 5. 動作要件

#### 5.1 環境要件
- Python 3.10以上（開発は 3.11 系仮想環境で確認）
- OpenAI APIキー（`.env` または `st.secrets`）
- メモリ: 1GB目安（データ量に依存）
- ストレージ: 1GB目安（`./FAISS_db` のサイズに依存）

#### 5.2 制限事項（現時点の実装上の仕様）
- 同時実行はStreamlitセッション依存（特段の制限ロジック無し）
- ドキュメント総容量・質問文字数・応答時間の上限制御は未実装
- 関連度スコアの表示は未実装


### 6. セキュリティ/運用
- **APIキー管理**: `.env`（ローカル）優先、無い場合は `st.secrets` を参照
- **ログ運用**: `TimedRotatingFileHandler` により日次ローテーション、セッションID付きで記録
- **文字コード対応**: Windows環境でのUnicode正規化およびcp932非対応文字の除去を実施


### 7. 既知の前提/非機能
- ネットワーク接続（OpenAI API・外部Web読込に必要）
- 初回起動時はデータ読み込み・埋め込み・FAISS保存のため時間を要する
